{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1217e70",
   "metadata": {},
   "source": [
    "# REINFORCE training with explainable rewards\n",
    "\n",
    "This notebook implements REINFORCE with baseline RLHF method and its training process with explainable reward model. For REINFORCE w/ baseline training `PyTorch` library used. Explainable reward model is implemented in `src/reward.py` like in the paper [Explainable Rewards in RLHF Using LLM-as-a-Judge](https://openreview.net/forum?id=FaOeBrlPst)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846e527e",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2fbc341",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "WARNING - Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, GenerationConfig, get_linear_schedule_with_warmup, BitsAndBytesConfig\n",
    "from peft import LoraConfig, PeftModel, get_peft_model\n",
    "import logging\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import wandb\n",
    "from huggingface_hub import login\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "from src import config, utils, reward\n",
    "from data import preprocess_helpsteer\n",
    "\n",
    "\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.manual_seed(config.SEED)\n",
    "random.seed(config.SEED)\n",
    "np.random.seed(config.SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "device = utils.get_device()\n",
    "\n",
    "login(token=config.HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e787ad7b",
   "metadata": {},
   "source": [
    "### Data loading and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a74010b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Loading and preparing RL dataset\n",
      "INFO - Loading dataset: nvidia/HelpSteer2\n",
      "INFO - Dataset loaded: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['prompt', 'response', 'helpfulness', 'correctness', 'coherence', 'complexity', 'verbosity'],\n",
      "        num_rows: 20324\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['prompt', 'response', 'helpfulness', 'correctness', 'coherence', 'complexity', 'verbosity'],\n",
      "        num_rows: 1038\n",
      "    })\n",
      "})\n",
      "INFO - Renamed prompt column to 'query'.\n",
      "INFO - RL Dataset prepared with 20324 training prompts and 1038 test prompts.\n",
      "INFO - Loaded 1000 training prompts.\n",
      "INFO - Created DataLoaders with batch size 1.\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Loading and preparing RL dataset\")\n",
    "try:\n",
    "    rl_dataset = preprocess_helpsteer.load_and_prepare_rl_dataset()\n",
    "    train_ds = rl_dataset[\"train\"].shuffle(seed=config.SEED).select(range(1000))\n",
    "    val_ds = rl_dataset[\"test\"].shuffle(seed=config.SEED).select(range(100))\n",
    "    # train_ds = rl_dataset[\"train\"]   # TO_DO: CHANGE TO BIGGER PARTITION FOR FULL TRAINING\n",
    "    logger.info(f\"Loaded {len(train_ds)} training prompts.\")\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        return {'query': [item['query'] for item in batch]}\n",
    "\n",
    "    dataloader_train = DataLoader(train_ds, batch_size=config.RL_BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "    dataloader_val = DataLoader(val_ds, batch_size=config.RL_BATCH_SIZE, collate_fn=collate_fn)\n",
    "    logger.info(f\"Created DataLoaders with batch size {config.RL_BATCH_SIZE}.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load dataset: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "805613c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['query'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a428f0b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['query'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c95a4864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What does OOPS as a development methodology stand for?\\n<extra_id_1>Assistant\\nOOPS is an acronym for Object-Oriented Programming with Nested Structures. OOPS is a software development methodology that emphasizes the use of objects, classes, and inheritance in software design. OOPS also supports the use of modular programming techniques and the divide-and-conquer algorithm for problem solving. OOPS has gained popularity as a software development methodology because it supports the creation of modular, reusable code that is well-suited for large software projects.\\n<extra_id_1>User\\nInvestigate any two the divide-and-conquer algorithms. Select one real life problem in which they are applicable. Implement the solution to the problem. Provide both theoretical and experimental assessment of the performance the algorithm.\\n<extra_id_1>Assistant\\nOne of the most common divide-and-conquer algorithms is the merge sort algorithm. The merge sort algorithm takes an array of items and recursively divides it into smaller sub-arrays, then combines the sub-arrays in a sorted order to produce the final sorted list. The merge sort algorithm is a very efficient sorting algorithm with an average time complexity of O(n log n). A real life problem that can be efficiently solved using the merge sort algorithm is the sorting of a large list of items.\\n<extra_id_1>User\\nNow implement it using Ruby\\n<extra_id_1>Assistant\\nThe merge sort algorithm can be implemented using a simple recursive Ruby function as follows:\\n \\n\\n def merge_sort(array)\\n  if array.length > 1\\n  mid = array.length / 2\\n  left_array = array[0...mid]\\n  right_array = array[mid...array.length]\\n \\n\\n  merge_sort(left_array)\\n  merge_sort(right_array)\\n \\n\\n  i = 0\\n  j = 0\\n  k = 0\\n \\n\\n  while i < left_array.length and j < right_array.length\\n  if left_array[i] < right_array[j]\\n  array[k] = left_array[i]\\n  i += 1\\n  else\\n  array[k] = right_array[j]\\n  j += 1\\n  end\\n  k += 1\\n  end\\n \\n\\n  while i < left_array.length\\n  array[k] = left_array[i]\\n  i += 1\\n  k += 1\\n  end\\n \\n\\n  while j < right_array.length\\n  array[k] = right_array[j]\\n  j += 1\\n  k += 1\\n  end\\n  end\\n end\\n<extra_id_1>User\\nList the divide and conquer algorithms\\n<extra_id_1>Assistant\\nThe following are some of the most common divide and conquer algorithms:\\n \\n\\n -  Merge sort\\n -  Binary search\\n -  Fast Fourier transform\\n -  Fast inverse Fourier transform\\n<extra_id_1>User\\nWhat is a divide and conquer algorithm?\\n<extra_id_1>Assistant\\nA divide and conquer algorithm is a problem-solving approach that solves a large problem by first dividing it into smaller subproblems, solving each subproblem, and then combining the solutions to the subproblems to solve the original problem. Divide and conquer algorithms are often used for sorting, searching, and other complex computational problems.\\n<extra_id_1>User\\nList the divide and conquer algorithsm'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbd79ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.BASE_MODEL_NAME)\n",
    "\n",
    "def tokenize_fn(examples):\n",
    "    tokens = tokenizer(\n",
    "        examples[\"query\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=config.MAX_SEQ_LENGTH,\n",
    "    )\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "348aeda6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = train_ds.map(tokenize_fn, batched=True, remove_columns=[\"query\"])\n",
    "train_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "val_ds = val_ds.map(tokenize_fn, batched=True, remove_columns=[\"query\"])\n",
    "val_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d87612c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(torch.utils.data.DataLoader(train_ds, batch_size=2)))\n",
    "print(type(batch[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1df199e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "riddled\n"
     ]
    }
   ],
   "source": [
    "# check that dataloader_train works\n",
    "batch = next(iter(dataloader_train))\n",
    "print(batch[\"query\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3bde14",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0eca7e8",
   "metadata": {},
   "source": [
    "#### Policy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb441db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Loading policy model for REINFORCE: Qwen/Qwen2.5-0.5B-Instruct\n",
      "INFO - Loading model: Qwen/Qwen2.5-0.5B-Instruct for mode: causal\n",
      "INFO - 4-bit quantization enabled.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "INFO - Prepared model for 4-bit training.\n",
      "INFO - Initializing new LoRA layers for training.\n",
      "INFO - Model and tokenizer loading complete.\n",
      "INFO - Policy model and tokenizer loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8,798,208 || all params: 502,830,976 || trainable%: 1.7497\n",
      "trainable params: 8,798,208 || all params: 502,830,976 || trainable%: 1.7497\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Loading policy model for REINFORCE: {config.BASE_MODEL_NAME}\")\n",
    "\n",
    "policy_model, tokenizer = utils.load_model_and_tokenizer(\n",
    "    config.BASE_MODEL_NAME,\n",
    "    load_4bit=True,\n",
    "    add_lora=True\n",
    ")\n",
    "\n",
    "logger.info(\"Policy model and tokenizer loaded.\")\n",
    "policy_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43ebd0d",
   "metadata": {},
   "source": [
    "#### Reference model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53f21ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Loading reference model: Qwen/Qwen2.5-0.5B-Instruct\n",
      "INFO - Loading model: Qwen/Qwen2.5-0.5B-Instruct for mode: causal\n",
      "INFO - 4-bit quantization enabled.\n",
      "INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "INFO - Prepared model for 4-bit training.\n",
      "INFO - Model and tokenizer loading complete.\n",
      "INFO - Reference model loaded.\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Loading reference model: {config.BASE_MODEL_NAME}\")\n",
    "ref_model, _ = utils.load_model_and_tokenizer(\n",
    "    config.BASE_MODEL_NAME,\n",
    "    load_4bit=True,\n",
    "    add_lora=False\n",
    ")\n",
    "\n",
    "ref_model.eval()\n",
    "logger.info(\"Reference model loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3b3ddb",
   "metadata": {},
   "source": [
    "#### Reward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "886a969b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Initializing Explainable Reward Model...\n",
      "INFO - Initializing explainable RM using judge: Qwen/Qwen2.5-0.5B-Instruct on device cuda\n",
      "INFO - Loading model: Qwen/Qwen2.5-0.5B-Instruct for mode: causal\n",
      "INFO - 4-bit quantization enabled.\n",
      "INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "INFO - Prepared model for 4-bit training.\n",
      "INFO - Model and tokenizer loading complete.\n",
      "INFO - Judge model loaded and set to evaluation mode.\n",
      "INFO - Explainable RM initialized.\n",
      "INFO - Reward model initialized.\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Initializing Explainable Reward Model...\")\n",
    "explainable_reward = reward.ExplainableRewardModel(model_name=config.JUDGE_MODEL_NAME, device=config.DEVICE)\n",
    "logger.info(\"Reward model initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4916fc48",
   "metadata": {},
   "source": [
    "### Training (REINFORCE w/ baseline implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4a8e2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_params = list(filter(lambda p: p.requires_grad, policy_model.parameters()))\n",
    "optimizer = AdamW(trainable_params, lr=config.REINFORCE_LEARNING_RATE)\n",
    "\n",
    "num_update_steps_per_epoch = len(dataloader_train) // config.REINFORCE_GRAD_ACCUMULATION\n",
    "total_training_steps = num_update_steps_per_epoch * config.REINFORCE_NUM_EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.03 * total_training_steps) if total_training_steps > 0 else 0, \n",
    "    num_training_steps=total_training_steps if total_training_steps > 0 else 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1693dfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovingAverageBaseline:\n",
    "    \"\"\"Simple exponential moving average baseline.\"\"\"\n",
    "    def __init__(self, initial_value=0.0, alpha=config.REINFORCE_BASELINE_ALPHA):\n",
    "        self.value = initial_value\n",
    "        self.alpha = alpha\n",
    "        self.initialized = False\n",
    "        self.steps = 0\n",
    "\n",
    "    def update(self, rewards: torch.Tensor):\n",
    "        batch_mean = rewards.mean().item()\n",
    "        if not self.initialized:\n",
    "            self.value = batch_mean\n",
    "            self.initialized = True\n",
    "        else:\n",
    "            # Exponential moving average\n",
    "            self.value = self.alpha * self.value + (1 - self.alpha) * batch_mean\n",
    "        self.steps += rewards.numel()\n",
    "\n",
    "    def get(self) -> float:\n",
    "        return self.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4a65e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = MovingAverageBaseline(alpha=config.REINFORCE_BASELINE_ALPHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a767a208",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Optimizer, Scheduler, Baseline, GradScaler initialized.\n",
      "INFO - Training for 1 epochs.\n",
      "INFO - Total expected optimizer steps: 500\n"
     ]
    }
   ],
   "source": [
    "scaler = torch.amp.GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "logger.info(f\"Optimizer, Scheduler, Baseline, GradScaler initialized.\")\n",
    "logger.info(f\"Training for {config.REINFORCE_NUM_EPOCHS} epochs.\")\n",
    "logger.info(f\"Total expected optimizer steps: {total_training_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37db1c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Starting REINFORCE training loop...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\milya\\_netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmiliusha2801\u001b[0m (\u001b[33mmiliusha2801-innopolis-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>m:\\python_projects\\interpretable_rewards_in_RLHF\\notebooks\\wandb\\run-20250429_125917-1d1fciv3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/miliusha2801-innopolis-university/xai-reinforce-explainable/runs/1d1fciv3' target=\"_blank\">reinforce-Qwen2.5-0.5B-Instruct</a></strong> to <a href='https://wandb.ai/miliusha2801-innopolis-university/xai-reinforce-explainable' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/miliusha2801-innopolis-university/xai-reinforce-explainable' target=\"_blank\">https://wandb.ai/miliusha2801-innopolis-university/xai-reinforce-explainable</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/miliusha2801-innopolis-university/xai-reinforce-explainable/runs/1d1fciv3' target=\"_blank\">https://wandb.ai/miliusha2801-innopolis-university/xai-reinforce-explainable/runs/1d1fciv3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - WandB initialized.\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Starting REINFORCE training loop...\")\n",
    "\n",
    "import wandb\n",
    "wandb.login(key=config.WANDB_API)\n",
    "\n",
    "if config.LOG_WITH == \"wandb\":\n",
    "    try:\n",
    "        run = wandb.init(\n",
    "            project=\"xai-reinforce-explainable\",\n",
    "            name=f\"reinforce-{config.BASE_MODEL_NAME.split('/')[-1]}\"\n",
    "        )\n",
    "        logger.info(\"WandB initialized.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize WandB: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9cbb7076",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=config.RL_MAX_NEW_TOKENS,\n",
    "    min_length=-1,\n",
    "    top_k=config.RL_TOP_K,\n",
    "    top_p=config.RL_TOP_P,\n",
    "    temperature=config.RL_TEMPERATURE,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dbc6c896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_responses(prompts, policy_model, tokenizer, generation_config, device):\n",
    "    \"\"\"Generates responses for a batch of prompts.\"\"\"\n",
    "    prompt_tokens = tokenizer(\n",
    "        prompts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=config.MAX_SEQ_LENGTH - config.RL_MAX_NEW_TOKENS\n",
    "    ).to(device)\n",
    "\n",
    "    response_ids_list = []\n",
    "    with torch.no_grad():\n",
    "        policy_model.eval()\n",
    "        try:\n",
    "            outputs = policy_model.generate(\n",
    "                input_ids=prompt_tokens.input_ids,\n",
    "                attention_mask=prompt_tokens.attention_mask,\n",
    "                generation_config=generation_config\n",
    "            )\n",
    "\n",
    "            if tokenizer.padding_side == \"left\":\n",
    "                response_ids_list = [ out[len(in_id):] for out, in_id in zip(outputs, prompt_tokens.input_ids) ]\n",
    "            else:\n",
    "                response_ids = outputs[:, prompt_tokens.input_ids.shape[1]:]\n",
    "                response_ids_list = [r for r in response_ids]\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during generation: {e}\", exc_info=True)\n",
    "            response_ids_list = [torch.tensor([], dtype=torch.long, device=device) for _ in prompts]\n",
    "        finally:\n",
    "            policy_model.train()\n",
    "\n",
    "    responses = tokenizer.batch_decode(response_ids_list, skip_special_tokens=True)\n",
    "    return prompt_tokens, response_ids_list, responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca9c0013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_log_probs(prompt_tokens, response_ids, policy_model, tokenizer, device):\n",
    "    \"\"\"Calculates log probabilities of generated sequences.\"\"\"\n",
    "    log_probs_list = []\n",
    "    batch_size = prompt_tokens.input_ids.shape[0]\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Combine prompt and response\n",
    "        single_prompt_ids = prompt_tokens.input_ids[i : i + 1]\n",
    "        single_response_ids = response_ids[i].unsqueeze(0)\n",
    "\n",
    "        if single_response_ids.numel() == 0:\n",
    "            log_probs_list.append(torch.tensor(0.0).to(device))\n",
    "            continue\n",
    "        full_ids = torch.cat([single_prompt_ids, single_response_ids], dim=1)\n",
    "\n",
    "        # Create attention mask for the full sequence\n",
    "        single_prompt_mask = prompt_tokens.attention_mask[i : i + 1]\n",
    "        response_mask = torch.ones_like(single_response_ids)\n",
    "        full_attention_mask = torch.cat([single_prompt_mask, response_mask], dim=1)\n",
    "\n",
    "        # Truncate\n",
    "        if full_ids.shape[1] > config.MAX_SEQ_LENGTH:\n",
    "            full_ids = full_ids[:, :config.MAX_SEQ_LENGTH]\n",
    "            full_attention_mask = full_attention_mask[:, :config.MAX_SEQ_LENGTH]\n",
    "\n",
    "        try:\n",
    "            with torch.autocast(device_type=config.DEVICE):\n",
    "                model_outputs = policy_model(input_ids=full_ids, attention_mask=full_attention_mask)\n",
    "                logits = model_outputs.logits\n",
    "\n",
    "            # Shift logits/labels for calculating log prob of response\n",
    "            start_index = single_prompt_ids.shape[1] - 1\n",
    "            end_index = full_ids.shape[1] - 1\n",
    "\n",
    "            if end_index <= start_index:\n",
    "                 log_probs_list.append(torch.tensor(0.0).to(device))\n",
    "                 continue\n",
    "\n",
    "            shift_logits = logits[:, start_index:end_index, :].contiguous()\n",
    "            shift_labels = full_ids[:, start_index+1:end_index+1].contiguous()\n",
    "\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "            log_probs_per_token = -loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1).long())\n",
    "\n",
    "            # Mask padding tokens within the response part\n",
    "            label_mask = (shift_labels != tokenizer.pad_token_id).float()\n",
    "            log_probs_per_token = log_probs_per_token.view(shift_labels.shape[0], -1) * label_mask\n",
    "            sequence_log_probs = log_probs_per_token.sum(dim=1)\n",
    "            log_probs_list.append(sequence_log_probs.squeeze())\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating log probs for sample {i}: {e}\", exc_info=True)\n",
    "            log_probs_list.append(torch.tensor(0.0).to(device))\n",
    "\n",
    "    if not log_probs_list:\n",
    "        return torch.tensor([], device=device)\n",
    "    return torch.stack(log_probs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f1ed471",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_kl_penalty(prompt_tokens, response_ids, policy_model, ref_model, tokenizer, device):\n",
    "    \"\"\"Calculates KL penalty between policy and reference model.\"\"\"\n",
    "    kl_penalties = []\n",
    "    batch_size = prompt_tokens.input_ids.shape[0]\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Combine prompt and response\n",
    "        single_prompt_ids = prompt_tokens.input_ids[i : i + 1]\n",
    "        single_response_ids = response_ids[i].unsqueeze(0)\n",
    "\n",
    "        if single_response_ids.numel() == 0:\n",
    "            kl_penalties.append(torch.tensor(0.0).to(device))\n",
    "            continue\n",
    "\n",
    "        full_ids = torch.cat([single_prompt_ids, single_response_ids], dim=1)\n",
    "        single_prompt_mask = prompt_tokens.attention_mask[i : i + 1]\n",
    "        response_mask = torch.ones_like(single_response_ids)\n",
    "        full_attention_mask = torch.cat([single_prompt_mask, response_mask], dim=1)\n",
    "\n",
    "        if full_ids.shape[1] > config.MAX_SEQ_LENGTH:\n",
    "            full_ids = full_ids[:, :config.MAX_SEQ_LENGTH]\n",
    "            full_attention_mask = full_attention_mask[:, :config.MAX_SEQ_LENGTH]\n",
    "\n",
    "        start_index = single_prompt_ids.shape[1] - 1\n",
    "        end_index = full_ids.shape[1] - 1\n",
    "\n",
    "        if end_index <= start_index:\n",
    "            kl_penalties.append(torch.tensor(0.0).to(device))\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            with torch.no_grad(), torch.autocast(device_type=config.DEVICE):\n",
    "                # Get policy logits\n",
    "                policy_outputs = policy_model(input_ids=full_ids, attention_mask=full_attention_mask)\n",
    "                policy_logits_shifted = policy_outputs.logits[:, start_index:end_index, :].contiguous()\n",
    "\n",
    "                # Get reference logits\n",
    "                ref_outputs = ref_model(input_ids=full_ids, attention_mask=full_attention_mask)\n",
    "                ref_logits_shifted = ref_outputs.logits[:, start_index:end_index, :].contiguous()\n",
    "\n",
    "                # Calculate KL\n",
    "                prob_policy = F.softmax(policy_logits_shifted, dim=-1)\n",
    "                log_prob_policy = F.log_softmax(policy_logits_shifted, dim=-1)\n",
    "                prob_ref = F.softmax(ref_logits_shifted, dim=-1)\n",
    "                log_prob_ref = F.log_softmax(ref_logits_shifted, dim=-1)\n",
    "\n",
    "                # Mask needs to match shifted logits shape\n",
    "                shift_labels = full_ids[:, start_index+1:end_index+1].contiguous()\n",
    "                label_mask = (shift_labels != tokenizer.pad_token_id).float()\n",
    "\n",
    "                if label_mask.shape != kl_div_per_token.shape:\n",
    "                     label_mask = label_mask.view_as(kl_div_per_token)\n",
    "\n",
    "                # KL divergence: sum_vocab(P_policy * (log P_policy - log P_ref))\n",
    "                kl_div_per_token = torch.sum(prob_policy * (log_prob_policy - log_prob_ref), dim=-1)\n",
    "                kl_div_per_token = kl_div_per_token * label_mask\n",
    "                kl_penalty = kl_div_per_token.sum(dim=1)\n",
    "                kl_penalties.append(kl_penalty.squeeze())\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating KL for sample {i}: {e}\", exc_info=True)\n",
    "            kl_penalties.append(torch.tensor(0.0).to(device))\n",
    "\n",
    "    if not kl_penalties:\n",
    "        return torch.tensor([], device=device)\n",
    "    return torch.stack(kl_penalties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0b9e088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_advantages(rewards, kl_penalty, baseline_value):\n",
    "    \"\"\"Applies KL penalty and calculates advantages.\"\"\"\n",
    "    final_rewards = rewards - config.KL_PENALTY_BETA * kl_penalty\n",
    "    advantages = final_rewards - baseline_value\n",
    "    return final_rewards, advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3734372d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce_loss_step(advantages, sequence_log_probs):\n",
    "    \"\"\"Calculates the REINFORCE loss for a batch.\"\"\"\n",
    "    loss = - (advantages.detach() * sequence_log_probs).mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b370814f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Starting REINFORCE training loop...\n",
      "INFO - Epoch 1/1\n",
      "Epoch 1 Batches:   0%|          | 0/1000 [00:00<?, ?it/s]ERROR - Error during generation: index -1 is out of bounds for dimension 0 with size 0\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\milya\\AppData\\Local\\Temp\\ipykernel_7916\\1816318076.py\", line 15, in generate_responses\n",
      "    outputs = policy_model.generate(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"m:\\python_projects\\interpretable_rewards_in_RLHF\\venv\\Lib\\site-packages\\peft\\peft_model.py\", line 1875, in generate\n",
      "    outputs = self.base_model.generate(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"m:\\python_projects\\interpretable_rewards_in_RLHF\\venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"m:\\python_projects\\interpretable_rewards_in_RLHF\\venv\\Lib\\site-packages\\transformers\\generation\\utils.py\", line 2465, in generate\n",
      "    result = self._sample(\n",
      "             ^^^^^^^^^^^^^\n",
      "  File \"m:\\python_projects\\interpretable_rewards_in_RLHF\\venv\\Lib\\site-packages\\transformers\\generation\\utils.py\", line 3424, in _sample\n",
      "    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"m:\\python_projects\\interpretable_rewards_in_RLHF\\venv\\Lib\\site-packages\\transformers\\generation\\utils.py\", line 507, in prepare_inputs_for_generation\n",
      "    inputs_embeds, input_ids = self._cache_dependant_input_preparation(\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"m:\\python_projects\\interpretable_rewards_in_RLHF\\venv\\Lib\\site-packages\\transformers\\generation\\utils.py\", line 406, in _cache_dependant_input_preparation\n",
      "    or (cache_position[-1] >= input_ids.shape[1])  # Exception 3\n",
      "        ~~~~~~~~~~~~~~^^^^\n",
      "IndexError: index -1 is out of bounds for dimension 0 with size 0\n",
      "WARNING - Skipping invalid prompt or response at index 0. Prompt: 'Web search results:\n",
      "\n",
      "[1] \"Download DeJ Loafs debut EP #AndSeeThatsTheThing here: http://smarturl.it/andseethatsthething Stream it here: http://smarturl.it/StreamASTTT Follow DeJ Loaf...\"\n",
      "Source: https://www.youtube.com/watch?v=r86-7UA6jaU\n",
      "\n",
      "[2] \"Answer (1 of 6): Hey would be used when you are trying to get someones attention. Technically it should be, Hey you, what are you doing? but it can be shortened to just Hey and still have the same meaning. Hey there is the equivalent to saying, whats up? Hi there is simply a greeting ...\"\n",
      "Source: https://www.quora.com/What-does-hey-there-mean?share=1\n",
      "\n",
      "[3] \"Hey There is a show tune from the musical play The Pajama Game, written by Richard Adler and Jerry Ross. It was published in 1954. It was introduced by John Raitt in the original production. In the show, Sid sings it to a recording device, telling himself that hes foolish to continue his advances to Babe. He plays the tape back, and after ...\"\n",
      "Source: https://en.wikipedia.org/wiki/Hey_There\n",
      "\n",
      "\n",
      "Current date: 12/19/2022\n",
      "Instructions: Using the provided web search results, write a comprehensive reply to the given prompt. Make sure to cite results using [[number](URL)] notation after the reference. If the provided search results refer to multiple subjects with the same name, write separate answers for each subject.\n",
      "Prompt: hey there', Response: ''. Assigning zero reward.\n",
      "WARNING - Reward shape mismatch (torch.Size([])) vs log_probs (torch.Size([1])). Adjusting.\n",
      "ERROR - Error getting reward for batch 0: Dimension specified as 0 but tensor has no dimensions\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\milya\\AppData\\Local\\Temp\\ipykernel_7916\\1763364058.py\", line 51, in <module>\n",
      "    aligned_rewards[:min_len] = rewards[:min_len]\n",
      "                                ~~~~~~~^^^^^^^^^^\n",
      "IndexError: Dimension specified as 0 but tensor has no dimensions\n",
      "Epoch 1 Batches:   0%|          | 0/1000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 65\u001b[39m\n\u001b[32m     63\u001b[39m loss = reinforce_loss_step(advantages, sequence_log_probs)\n\u001b[32m     64\u001b[39m loss_scaled = loss / config.REINFORCE_GRAD_ACCUMULATION\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_scaled\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m step_accumulation_counter += \u001b[32m1\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# Logging\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mm:\\python_projects\\interpretable_rewards_in_RLHF\\venv\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mm:\\python_projects\\interpretable_rewards_in_RLHF\\venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mm:\\python_projects\\interpretable_rewards_in_RLHF\\venv\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "logger.info(\"Starting REINFORCE training loop...\")\n",
    "\n",
    "global_step = 0\n",
    "policy_model.train()\n",
    "\n",
    "for epoch in range(config.REINFORCE_NUM_EPOCHS):\n",
    "    logger.info(f\"Epoch {epoch+1}/{config.REINFORCE_NUM_EPOCHS}\")\n",
    "    policy_model.train()\n",
    "\n",
    "    progress_bar = tqdm(dataloader_train, desc=f\"Epoch {epoch+1} Batches\")\n",
    "    step_accumulation_counter = 0\n",
    "    epoch_metrics = {\n",
    "        \"loss\": [], \"reward_raw_mean\": [], \"kl_penalty_mean\": [],\n",
    "        \"final_reward_mean\": [], \"advantage_mean\": [], \"log_prob_mean\": []\n",
    "    }\n",
    "\n",
    "    for i, batch in enumerate(progress_bar):\n",
    "        start_time_batch = time.time()\n",
    "        prompts = batch[\"query\"]\n",
    "\n",
    "        # Generate responses\n",
    "        prompt_tokens, response_ids_list, responses = generate_responses(\n",
    "            prompts, policy_model, tokenizer, generation_config, device\n",
    "        )\n",
    "\n",
    "        # Calculate logprobs\n",
    "        sequence_log_probs = calculate_log_probs(\n",
    "            prompt_tokens, response_ids_list, policy_model, tokenizer, device\n",
    "        )\n",
    "\n",
    "        # Handle cases where log_prob calculation failed (if returned empty tensor)\n",
    "        if sequence_log_probs.numel() == 0:\n",
    "            logger.warning(f\"Skipping batch {i} due to log_prob calculation errors.\")\n",
    "            continue\n",
    "\n",
    "        # Calculate KL penalty\n",
    "        kl_penalty = calculate_kl_penalty(\n",
    "            prompt_tokens, response_ids_list, policy_model, ref_model, tokenizer, device\n",
    "        )\n",
    "        if kl_penalty.numel() == 0:\n",
    "             kl_penalty = torch.zeros_like(sequence_log_probs)\n",
    "\n",
    "        # Compute rewards\n",
    "        try:\n",
    "            rewards = explainable_reward(prompts, responses)\n",
    "            rewards = rewards.to(device).squeeze()\n",
    "            if rewards.shape != sequence_log_probs.shape:\n",
    "                 logger.warning(f\"Reward shape mismatch ({rewards.shape}) vs log_probs ({sequence_log_probs.shape}). Adjusting.\")\n",
    "                 aligned_rewards = torch.zeros_like(sequence_log_probs)\n",
    "                 min_len = min(rewards.numel(), sequence_log_probs.numel())\n",
    "                 aligned_rewards[:min_len] = rewards[:min_len]\n",
    "                 rewards = aligned_rewards\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error getting reward for batch {i}: {e}\", exc_info=True)\n",
    "            rewards = torch.zeros_like(sequence_log_probs)\n",
    "\n",
    "        # Compute advantages\n",
    "        current_baseline_value = baseline.get()\n",
    "        final_rewards, advantages = compute_advantages(rewards, kl_penalty, current_baseline_value)\n",
    "\n",
    "        # Calculate loss and backpropagate\n",
    "        loss = reinforce_loss_step(advantages, sequence_log_probs)\n",
    "        loss_scaled = loss / config.REINFORCE_GRAD_ACCUMULATION\n",
    "        scaler.scale(loss_scaled).backward()\n",
    "\n",
    "        step_accumulation_counter += 1\n",
    "\n",
    "        # Logging\n",
    "        epoch_metrics[\"loss\"].append(loss.item())\n",
    "        epoch_metrics[\"reward_raw_mean\"].append(rewards.mean().item())\n",
    "        epoch_metrics[\"kl_penalty_mean\"].append(kl_penalty.mean().item())\n",
    "        epoch_metrics[\"final_reward_mean\"].append(final_rewards.mean().item())\n",
    "        epoch_metrics[\"advantage_mean\"].append(advantages.mean().item())\n",
    "        epoch_metrics[\"log_prob_mean\"].append(sequence_log_probs.mean().item())\n",
    "\n",
    "        # Optimizer Step\n",
    "        if step_accumulation_counter % config.REINFORCE_GRAD_ACCUMULATION == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(trainable_params, 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            step_time = time.time() - start_time_batch\n",
    "            avg_metrics = {k: np.mean(v) for k, v in epoch_metrics.items()}\n",
    "            baseline.update(torch.tensor(epoch_metrics[\"final_reward_mean\"], device=device))\n",
    "\n",
    "            logs = {\n",
    "                f\"train/{k}\": v for k, v in avg_metrics.items()\n",
    "            }\n",
    "            logs.update({\n",
    "                \"train/baseline\": baseline.get(),\n",
    "                \"train/learning_rate\": scheduler.get_last_lr()[0],\n",
    "                \"train/step_time_s\": step_time,\n",
    "                \"global_step\": global_step + 1,\n",
    "                \"epoch\": epoch + (i+1)/len(dataloader_train)\n",
    "            })\n",
    "\n",
    "            progress_bar.set_postfix({\n",
    "                \"Reward\": f\"{avg_metrics['final_reward_mean']:.3f}\",\n",
    "                \"Loss\": f\"{avg_metrics['loss']:.3f}\",\n",
    "                \"KL\": f\"{avg_metrics['kl_penalty_mean']:.3f}\",\n",
    "                \"Adv\": f\"{avg_metrics['advantage_mean']:.3f}\"\n",
    "            })\n",
    "\n",
    "            if run:\n",
    "                try:\n",
    "                    wandb.log(logs)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Failed to log metrics to WandB at step {global_step}: {e}\")\n",
    "\n",
    "            epoch_metrics = {k: [] for k in epoch_metrics}\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "            # Save checkpoint\n",
    "            if global_step % config.SAVE_FREQ == 0:\n",
    "                logger.info(f\"Saving checkpoint at step {global_step}...\")\n",
    "                local_save_path = f\"{config.OUTPUT_DIR}/reinforce/checkpoint-{global_step}\"\n",
    "                try:\n",
    "                    os.makedirs(os.path.dirname(local_save_path), exist_ok=True)\n",
    "                    policy_model.save_pretrained(local_save_path)\n",
    "                    tokenizer.save_pretrained(local_save_path)\n",
    "                    logger.info(f\"REINFORCE checkpoint saved locally to {local_save_path}\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Failed to save checkpoint at step {global_step}: {e}\")\n",
    "\n",
    "            step_accumulation_counter = 0\n",
    "\n",
    "    # Validation\n",
    "    logger.info(f\"Validation for epoch {epoch+1}\")\n",
    "    policy_model.eval()\n",
    "    total_val_reward = 0.0\n",
    "    total_val_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for val_batch in tqdm(dataloader_val, desc=f\"Validation epoch {epoch+1}\", leave=False):\n",
    "            val_prompts = val_batch[\"query\"]\n",
    "            # Generate responses for validation\n",
    "            _, _, val_responses = generate_responses(\n",
    "                val_prompts, policy_model, tokenizer, generation_config, device\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                # Score responses\n",
    "                val_rewards = explainable_reward(val_prompts, val_responses)\n",
    "                total_val_reward += val_rewards.sum().item()\n",
    "                total_val_samples += val_rewards.numel()\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error getting validation reward: {e}\")\n",
    "\n",
    "    avg_val_reward = total_val_reward / total_val_samples if total_val_samples > 0 else 0\n",
    "    logger.info(f\"Epoch {epoch+1} Validation average reward: {avg_val_reward:.4f}\")\n",
    "\n",
    "    if run:\n",
    "        try:\n",
    "            wandb.log({\"validation/avg_reward\": avg_val_reward, \"epoch\": epoch + 1, \"global_step\": global_step})\n",
    "        except Exception as e:\n",
    "             logger.error(f\"Failed to log validation metrics to WandB: {e}\")\n",
    "\n",
    "logger.info(\"REINFORCE Training loop finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb35592",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3344f310",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
