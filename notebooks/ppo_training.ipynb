{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0917cff4",
   "metadata": {},
   "source": [
    "# PPO training with explainable rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8ee3f0",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dc10e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "WARNING - Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "INFO - Logged into Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, GenerationConfig, BitsAndBytesConfig\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead, setup_chat_format\n",
    "from peft import LoraConfig, PeftModel\n",
    "import logging\n",
    "import time\n",
    "# from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import wandb\n",
    "from huggingface_hub import login\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from src import config, utils, reward\n",
    "from data import preprocess_helpsteer\n",
    "\n",
    "\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "torch.manual_seed(config.SEED)\n",
    "random.seed(config.SEED)\n",
    "np.random.seed(config.SEED)\n",
    "\n",
    "if config.HF_TOKEN:\n",
    "    login(token=config.HF_TOKEN)\n",
    "    logger.info(\"Logged into Hugging Face Hub.\")\n",
    "else:\n",
    "    logger.warning(\"HF_TOKEN not found. Cannot push models to Hub.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a63100",
   "metadata": {},
   "source": [
    "### Data loading and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf6e039e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Loading and preparing RL dataset\n",
      "INFO - Loading dataset: nvidia/HelpSteer2\n",
      "INFO - Dataset loaded: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['prompt', 'response', 'helpfulness', 'correctness', 'coherence', 'complexity', 'verbosity'],\n",
      "        num_rows: 20324\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['prompt', 'response', 'helpfulness', 'correctness', 'coherence', 'complexity', 'verbosity'],\n",
      "        num_rows: 1038\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74494e4a20654ef9ab84e291fe37aa5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/20324 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db41a1b542194637bdb88caa4bfc72b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1038 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Renamed prompt column to 'query'.\n",
      "INFO - RL Dataset prepared with 20324 training prompts and 1038 test prompts.\n",
      "INFO - Loaded 1000 training prompts.\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Loading and preparing RL dataset\")\n",
    "\n",
    "try:\n",
    "    rl_dataset = preprocess_helpsteer.load_and_prepare_rl_dataset()\n",
    "    train_ds = rl_dataset[\"train\"].shuffle(seed=config.SEED).select(range(1000))\n",
    "    val_ds = rl_dataset[\"test\"].shuffle(seed=config.SEED).select(range(100))\n",
    "    # train_ds = rl_dataset[\"train\"]\n",
    "    logger.info(f\"Loaded {len(train_ds)} training prompts.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load dataset: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9473a235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['query'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c554468d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['query'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7a9ee89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What does OOPS as a development methodology stand for?\\n<extra_id_1>Assistant\\nOOPS is an acronym for Object-Oriented Programming with Nested Structures. OOPS is a software development methodology that emphasizes the use of objects, classes, and inheritance in software design. OOPS also supports the use of modular programming techniques and the divide-and-conquer algorithm for problem solving. OOPS has gained popularity as a software development methodology because it supports the creation of modular, reusable code that is well-suited for large software projects.\\n<extra_id_1>User\\nInvestigate any two the divide-and-conquer algorithms. Select one real life problem in which they are applicable. Implement the solution to the problem. Provide both theoretical and experimental assessment of the performance the algorithm.\\n<extra_id_1>Assistant\\nOne of the most common divide-and-conquer algorithms is the merge sort algorithm. The merge sort algorithm takes an array of items and recursively divides it into smaller sub-arrays, then combines the sub-arrays in a sorted order to produce the final sorted list. The merge sort algorithm is a very efficient sorting algorithm with an average time complexity of O(n log n). A real life problem that can be efficiently solved using the merge sort algorithm is the sorting of a large list of items.\\n<extra_id_1>User\\nNow implement it using Ruby\\n<extra_id_1>Assistant\\nThe merge sort algorithm can be implemented using a simple recursive Ruby function as follows:\\n \\n\\n def merge_sort(array)\\n  if array.length > 1\\n  mid = array.length / 2\\n  left_array = array[0...mid]\\n  right_array = array[mid...array.length]\\n \\n\\n  merge_sort(left_array)\\n  merge_sort(right_array)\\n \\n\\n  i = 0\\n  j = 0\\n  k = 0\\n \\n\\n  while i < left_array.length and j < right_array.length\\n  if left_array[i] < right_array[j]\\n  array[k] = left_array[i]\\n  i += 1\\n  else\\n  array[k] = right_array[j]\\n  j += 1\\n  end\\n  k += 1\\n  end\\n \\n\\n  while i < left_array.length\\n  array[k] = left_array[i]\\n  i += 1\\n  k += 1\\n  end\\n \\n\\n  while j < right_array.length\\n  array[k] = right_array[j]\\n  j += 1\\n  k += 1\\n  end\\n  end\\n end\\n<extra_id_1>User\\nList the divide and conquer algorithms\\n<extra_id_1>Assistant\\nThe following are some of the most common divide and conquer algorithms:\\n \\n\\n -  Merge sort\\n -  Binary search\\n -  Fast Fourier transform\\n -  Fast inverse Fourier transform\\n<extra_id_1>User\\nWhat is a divide and conquer algorithm?\\n<extra_id_1>Assistant\\nA divide and conquer algorithm is a problem-solving approach that solves a large problem by first dividing it into smaller subproblems, solving each subproblem, and then combining the solutions to the subproblems to solve the original problem. Divide and conquer algorithms are often used for sorting, searching, and other complex computational problems.\\n<extra_id_1>User\\nList the divide and conquer algorithsm'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab423fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.BASE_MODEL_NAME)\n",
    "\n",
    "def tokenize_fn(examples):\n",
    "    tokens = tokenizer(\n",
    "        examples[\"query\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=config.MAX_SEQ_LENGTH,\n",
    "    )\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc087626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90f7c785af5247f89c6729b28156a7a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c29a38abd0f54d79af841cb90fcbb9c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = train_ds.map(tokenize_fn, batched=True, remove_columns=[\"query\"])\n",
    "train_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "val_ds = val_ds.map(tokenize_fn, batched=True, remove_columns=[\"query\"])\n",
    "val_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89bb0aac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  6023,  38971,    678,   3681,  55090, 151643, 151643, 151643, 151643,\n",
       "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0][\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1df523e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(torch.utils.data.DataLoader(train_ds, batch_size=2)))\n",
    "print(type(batch[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598848bf",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f78934",
   "metadata": {},
   "source": [
    "#### Policy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bcac575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger.info(f\"Loading policy model: {config.BASE_MODEL_NAME}\")\n",
    "\n",
    "# lora_config_ppo = LoraConfig(\n",
    "#     r=config.LORA_R,\n",
    "#     lora_alpha=config.LORA_ALPHA,\n",
    "#     lora_dropout=config.LORA_DROPOUT,\n",
    "#     target_modules=config.LORA_TARGET_MODULES,\n",
    "#     bias=\"none\",\n",
    "#     task_type=\"CAUSAL_LM\",\n",
    "# )\n",
    "\n",
    "# policy_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "#     config.BASE_MODEL_NAME,\n",
    "#     trust_remote_code=True,\n",
    "#     load_in_4bit=True,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     device_map=\"auto\",\n",
    "#     peft_config=lora_config_ppo,\n",
    "# )\n",
    "# logger.info(\"Policy model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "454b59cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import GenerationConfig\n",
    "\n",
    "# if not hasattr(policy_model, \"generation_config\"):\n",
    "#     # You can load defaults from the base model or craft your own:\n",
    "#     policy_model.generation_config = GenerationConfig.from_pretrained(\n",
    "#         config.BASE_MODEL_NAME\n",
    "#     )  # controls generate() behavior :contentReference[oaicite:0]{index=0}\n",
    "\n",
    "# # 2. Tell PPO what your end‐of‐generation token is:\n",
    "# # ppo_config.stop_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77f0bedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Loading policy model: Qwen/Qwen2.5-0.5B-Instruct\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "INFO - peft adapter initialised\n",
      "WARNING - A <class 'peft.peft_model.PeftModelForCausalLM'> model is loaded from 'Qwen/Qwen2.5-0.5B-Instruct', and no v_head weight is found. This IS expected if you are not resuming PPO training.\n",
      "INFO - Policy model loaded.\n"
     ]
    }
   ],
   "source": [
    "from trl.models.modeling_value_head import AutoModelForCausalLMWithValueHead\n",
    "\n",
    "logger.info(f\"Loading policy model: {config.BASE_MODEL_NAME}\")\n",
    "\n",
    "lora_config_ppo = LoraConfig(\n",
    "    r=config.LORA_R,\n",
    "    lora_alpha=config.LORA_ALPHA,\n",
    "    lora_dropout=config.LORA_DROPOUT,\n",
    "    target_modules=config.LORA_TARGET_MODULES,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "policy_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    config.BASE_MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    load_in_4bit=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    peft_config=lora_config_ppo,\n",
    ")\n",
    "logger.info(\"Policy model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e3662d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "policy_model.config.use_cache = False\n",
    "policy_model.gradient_checkpointing_disable()\n",
    "\n",
    "if not hasattr(policy_model, \"generation_config\"):\n",
    "    policy_model.generation_config = GenerationConfig.from_pretrained(config.BASE_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756b0e2b",
   "metadata": {},
   "source": [
    "#### Reference model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b97d510b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger.info(f\"Loading reference model: {config.BASE_MODEL_NAME}\")\n",
    "\n",
    "# ref_model, tokenizer = utils.load_model_and_tokenizer(\n",
    "#     config.BASE_MODEL_NAME,\n",
    "#     load_4bit=True,\n",
    "#     add_lora=False\n",
    "# )\n",
    "# ref_model.eval()\n",
    "# logger.info(\"Reference model and tokenizer loaded.\")\n",
    "\n",
    "# if tokenizer.pad_token is None:\n",
    "#     tokenizer.pad_token = tokenizer.eos_token\n",
    "#     policy_model.config.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cce89b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "WARNING - A <class 'transformers.models.qwen2.modeling_qwen2.Qwen2ForCausalLM'> model is loaded from 'Qwen/Qwen2.5-0.5B-Instruct', and no v_head weight is found. This IS expected if you are not resuming PPO training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "from trl.models.modeling_value_head import AutoModelForCausalLMWithValueHead\n",
    "\n",
    "\n",
    "bnb_cfg = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    config.BASE_MODEL_NAME,\n",
    "    quantization_config=bnb_cfg,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "ref_model.eval()\n",
    "ref_model.config.use_cache = False\n",
    "ref_model.gradient_checkpointing_disable()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10211398",
   "metadata": {},
   "source": [
    "#### Reward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37b015fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Initializing Explainable Reward Model...\n",
      "INFO - Initializing explainable RM using judge: Qwen/Qwen2.5-0.5B-Instruct on device cuda\n",
      "INFO - Loading model: Qwen/Qwen2.5-0.5B-Instruct for mode: causal\n",
      "INFO - 4-bit quantization enabled.\n",
      "INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "INFO - Prepared model for 4-bit training.\n",
      "INFO - Model and tokenizer loading complete.\n",
      "INFO - Judge model loaded and set to evaluation mode.\n",
      "INFO - Explainable RM initialized.\n",
      "INFO - Reward model initialized.\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Initializing Explainable Reward Model...\")\n",
    "explainable_reward = reward.ExplainableRewardModel(model_name=config.JUDGE_MODEL_NAME, device=config.DEVICE)\n",
    "logger.info(\"Reward model initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cf294c",
   "metadata": {},
   "source": [
    "### Value model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6566a205",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Loading value model: Qwen/Qwen2.5-0.5B-Instruct\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Some weights of Qwen2ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen2.5-0.5B-Instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO - Value model loaded.\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Loading value model: {config.BASE_MODEL_NAME}\")\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "value_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    config.BASE_MODEL_NAME,\n",
    "    num_labels=1,\n",
    "    trust_remote_code=True,\n",
    "    load_in_4bit=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "value_model.eval()\n",
    "logger.info(\"Value model loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a130c98",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e33b1226",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_config = PPOConfig(\n",
    "    num_ppo_epochs=config.PPO_NUM_EPOCHS,\n",
    "    learning_rate=config.PPO_LEARNING_RATE,\n",
    "    report_to=config.LOG_WITH if config.LOG_WITH else None,\n",
    "    batch_size=config.RL_BATCH_SIZE,\n",
    "    mini_batch_size=config.PPO_MINI_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=config.PPO_GRAD_ACCUMULATION_PPO,\n",
    "    stop_token_id=tokenizer.eos_token_id,\n",
    "    seed=config.SEED,\n",
    "    logging_dir=\"ppo-runs/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25dc5272",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_trainer = PPOTrainer(\n",
    "    args=ppo_config,\n",
    "    model=policy_model,\n",
    "    ref_model=ref_model,\n",
    "    reward_model=explainable_reward,\n",
    "    value_model=value_model,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    data_collator=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f1b2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Starting PPO training loop...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmiliusha2801\u001b[0m (\u001b[33mmiliusha2801-innopolis-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>m:\\python_projects\\interpretable_rewards_in_RLHF\\notebooks\\wandb\\run-20250427_140257-v8pyfyo5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/miliusha2801-innopolis-university/xai-ppo-explainable/runs/v8pyfyo5' target=\"_blank\">ppo-Qwen2.5-0.5B-Instruct</a></strong> to <a href='https://wandb.ai/miliusha2801-innopolis-university/xai-ppo-explainable' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/miliusha2801-innopolis-university/xai-ppo-explainable' target=\"_blank\">https://wandb.ai/miliusha2801-innopolis-university/xai-ppo-explainable</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/miliusha2801-innopolis-university/xai-ppo-explainable/runs/v8pyfyo5' target=\"_blank\">https://wandb.ai/miliusha2801-innopolis-university/xai-ppo-explainable/runs/v8pyfyo5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - WandB initialized.\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Starting PPO training loop...\")\n",
    "\n",
    "import wandb\n",
    "wandb_api = \"86cd74d37ebed39035c6b54365fe1b6a76f36839\"\n",
    "wandb.login(key=wandb_api)\n",
    "\n",
    "if config.LOG_WITH == \"wandb\":\n",
    "    try:\n",
    "        wandb.init(\n",
    "            project=\"xai-ppo-explainable\",\n",
    "            name=f\"ppo-{config.BASE_MODEL_NAME.split('/')[-1]}\"\n",
    "        )\n",
    "        logger.info(\"WandB initialized.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize WandB: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "acc98ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_kwargs = {\n",
    "    \"max_new_tokens\": config.RL_MAX_NEW_TOKENS,\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": config.RL_TOP_K,\n",
    "    \"top_p\": config.RL_TOP_P,\n",
    "    \"temperature\": config.RL_TEMPERATURE,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "548afbf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===training policy===\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 10.23 GiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 9.15 GiB is allocated by PyTorch, and 356.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mppo_trainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mm:\\python_projects\\interpretable_rewards_in_RLHF\\venv\\Lib\\site-packages\\trl\\trainer\\ppo_trainer.py:440\u001b[39m, in \u001b[36mPPOTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    438\u001b[39m         ref_output = forward(model.policy, query_response, processing_class.pad_token_id)\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     ref_output = \u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mref_policy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_response\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    441\u001b[39m ref_logits = ref_output.logits[:, context_length - \u001b[32m1\u001b[39m : -\u001b[32m1\u001b[39m]\n\u001b[32m    442\u001b[39m ref_logits /= args.temperature + \u001b[32m1e-7\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mm:\\python_projects\\interpretable_rewards_in_RLHF\\venv\\Lib\\site-packages\\trl\\trainer\\utils.py:1231\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, query_responses, pad_token_id)\u001b[39m\n\u001b[32m   1229\u001b[39m position_ids = attention_mask.cumsum(\u001b[32m1\u001b[39m) - attention_mask.long()\n\u001b[32m   1230\u001b[39m input_ids = torch.masked_fill(query_responses, ~attention_mask, \u001b[32m0\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1231\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1232\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1237\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mm:\\python_projects\\interpretable_rewards_in_RLHF\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mm:\\python_projects\\interpretable_rewards_in_RLHF\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1845\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1842\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m   1844\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1845\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1846\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1847\u001b[39m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[32m   1848\u001b[39m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[32m   1849\u001b[39m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[32m   1850\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks.items():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mm:\\python_projects\\interpretable_rewards_in_RLHF\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1793\u001b[39m, in \u001b[36mModule._call_impl.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1790\u001b[39m     bw_hook = BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[32m   1791\u001b[39m     args = bw_hook.setup_input_hook(args)\n\u001b[32m-> \u001b[39m\u001b[32m1793\u001b[39m result = \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1794\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks:\n\u001b[32m   1795\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[32m   1796\u001b[39m         *_global_forward_hooks.items(),\n\u001b[32m   1797\u001b[39m         *\u001b[38;5;28mself\u001b[39m._forward_hooks.items(),\n\u001b[32m   1798\u001b[39m     ):\n\u001b[32m   1799\u001b[39m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mm:\\python_projects\\interpretable_rewards_in_RLHF\\venv\\Lib\\site-packages\\trl\\models\\modeling_value_head.py:180\u001b[39m, in \u001b[36mAutoModelForCausalLMWithValueHead.forward\u001b[39m\u001b[34m(self, input_ids, past_key_values, attention_mask, return_past_key_values, **kwargs)\u001b[39m\n\u001b[32m    178\u001b[39m \u001b[38;5;66;03m# force upcast in fp32 if logits are in half-precision\u001b[39;00m\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m lm_logits.dtype != torch.float32:\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     lm_logits = \u001b[43mlm_logits\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_past_key_values:\n\u001b[32m    183\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (lm_logits, loss, value, base_model_output.past_key_values)\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 10.23 GiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 9.15 GiB is allocated by PyTorch, and 356.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "ppo_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9935465",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a16585f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf6b1ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3564a426",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92b98bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
